{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ds/anaconda/lib/python2.7/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import metrics\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('/Users/ds/Documents/datascience/final_project/final_train_data/FINAL-TRAIN.csv', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>scores</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Families caught in the grip of the opioid cris...</td>\n",
       "      <td>{u'neg': 0.376, u'neu': 0.624, u'pos': 0.0, u'...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Wisconsin needs action from Washington to comb...</td>\n",
       "      <td>{u'neg': 0.356, u'neu': 0.644, u'pos': 0.0, u'...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I'm heartbroken to see WV families &amp;amp; towns...</td>\n",
       "      <td>{u'neg': 0.323, u'neu': 0.562, u'pos': 0.115, ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>#SmoggySkiesAct allows corporate polluters to ...</td>\n",
       "      <td>{u'neg': 0.453, u'neu': 0.547, u'pos': 0.0, u'...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>#Fakenews that was true:\\n\\nFlynn lied to VP\\n...</td>\n",
       "      <td>{u'neg': 0.341, u'neu': 0.574, u'pos': 0.085, ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               tweet  \\\n",
       "0  Families caught in the grip of the opioid cris...   \n",
       "1  Wisconsin needs action from Washington to comb...   \n",
       "2  I'm heartbroken to see WV families &amp; towns...   \n",
       "3  #SmoggySkiesAct allows corporate polluters to ...   \n",
       "4  #Fakenews that was true:\\n\\nFlynn lied to VP\\n...   \n",
       "\n",
       "                                              scores  sentiment  \n",
       "0  {u'neg': 0.376, u'neu': 0.624, u'pos': 0.0, u'...          1  \n",
       "1  {u'neg': 0.356, u'neu': 0.644, u'pos': 0.0, u'...          1  \n",
       "2  {u'neg': 0.323, u'neu': 0.562, u'pos': 0.115, ...          1  \n",
       "3  {u'neg': 0.453, u'neu': 0.547, u'pos': 0.0, u'...          1  \n",
       "4  {u'neg': 0.341, u'neu': 0.574, u'pos': 0.085, ...          2  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2    1528\n",
       "3     811\n",
       "1     661\n",
       "Name: sentiment, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sentiment.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = df.tweet\n",
    "y = df.sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tokenize_test(vect):\n",
    "    X_train_dtm = vect.fit_transform(X_train)\n",
    "    print('Features: ', X_train_dtm.shape[1])\n",
    "    X_test_dtm = vect.transform(X_test)\n",
    "    nb = MultinomialNB()\n",
    "    nb.fit(X_train_dtm, y_train)\n",
    "    y_pred_class = nb.predict(X_test_dtm)\n",
    "    print('Accuracy: ', metrics.accuracy_score(y_test, y_pred_class))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Features: ', 12525)\n",
      "('Accuracy: ', 0.61833333333333329)\n"
     ]
    }
   ],
   "source": [
    "vect = CountVectorizer(ngram_range=(1, 3), min_df=2)\n",
    "tokenize_test(vect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Features: ', 12457)\n",
      "('Accuracy: ', 0.64500000000000002)\n"
     ]
    }
   ],
   "source": [
    "vect = CountVectorizer()\n",
    "tokenize_test(vect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Features: ', 12205)\n",
      "('Accuracy: ', 0.65166666666666662)\n"
     ]
    }
   ],
   "source": [
    "vect = CountVectorizer(stop_words='english')\n",
    "tokenize_test(vect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Features: ', 55282)\n",
      "('Accuracy: ', 0.65333333333333332)\n"
     ]
    }
   ],
   "source": [
    "vect = CountVectorizer(ngram_range=(1, 2), max_features=100000)\n",
    "tokenize_test(vect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Features: ', 9766)\n",
      "('Accuracy: ', 0.6216666666666667)\n"
     ]
    }
   ],
   "source": [
    "vect = CountVectorizer(ngram_range=(1, 2), min_df=2)\n",
    "tokenize_test(vect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pos_neg = df[(df.sentiment==3) | (df.sentiment==1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = pos_neg.tweet\n",
    "y = pos_neg.sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Features: ', 6079)\n",
      "('Accuracy: ', 0.82711864406779656)\n"
     ]
    }
   ],
   "source": [
    "vect = CountVectorizer(ngram_range=(1, 3), min_df=2)\n",
    "tokenize_test(vect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Features: ', 7630)\n",
      "('Accuracy: ', 0.86101694915254234)\n"
     ]
    }
   ],
   "source": [
    "vect = CountVectorizer()\n",
    "tokenize_test(vect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Features: ', 7395)\n",
      "('Accuracy: ', 0.83389830508474572)\n"
     ]
    }
   ],
   "source": [
    "vect = CountVectorizer(stop_words='english')\n",
    "tokenize_test(vect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Features: ', 30678)\n",
      "('Accuracy: ', 0.84067796610169487)\n"
     ]
    }
   ],
   "source": [
    "vect = CountVectorizer(ngram_range=(1, 2), max_features=100000)\n",
    "tokenize_test(vect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Features: ', 5009)\n",
      "('Accuracy: ', 0.83389830508474572)\n"
     ]
    }
   ],
   "source": [
    "vect = CountVectorizer(ngram_range=(1, 2), min_df=2)\n",
    "tokenize_test(vect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.522033898305\n"
     ]
    }
   ],
   "source": [
    "y_test_binary = np.where(y_test==3, 1, 0)\n",
    "print(y_test_binary.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pos_sent = []\n",
    "for x in df.sentiment:\n",
    "    if x == 1 or x== 2:\n",
    "        pos_sent.append(1)\n",
    "    else:\n",
    "        pos_sent.append(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df['pos_sent'] = pos_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = df.tweet\n",
    "y = df.pos_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Features: ', 12525)\n",
      "('Accuracy: ', 0.79333333333333333)\n"
     ]
    }
   ],
   "source": [
    "vect = CountVectorizer(ngram_range=(1, 3), min_df=2)\n",
    "tokenize_test(vect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Features: ', 12457)\n",
      "('Accuracy: ', 0.81166666666666665)\n"
     ]
    }
   ],
   "source": [
    "vect = CountVectorizer()\n",
    "tokenize_test(vect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Features: ', 12205)\n",
      "('Accuracy: ', 0.82166666666666666)\n"
     ]
    }
   ],
   "source": [
    "vect = CountVectorizer(stop_words='english')\n",
    "tokenize_test(vect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Features: ', 55282)\n",
      "('Accuracy: ', 0.81999999999999995)\n"
     ]
    }
   ],
   "source": [
    "vect = CountVectorizer(ngram_range=(1, 2), max_features=100000)\n",
    "tokenize_test(vect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Features: ', 9766)\n",
      "('Accuracy: ', 0.80166666666666664)\n"
     ]
    }
   ],
   "source": [
    "vect = CountVectorizer(ngram_range=(1, 2), min_df=2)\n",
    "tokenize_test(vect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.733333333333\n"
     ]
    }
   ],
   "source": [
    "y_test_binary = np.where(y_test==1, 1, 0)\n",
    "print(y_test_binary.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "neg_sent = []\n",
    "for x in df.sentiment:\n",
    "    if x == 1:\n",
    "        neg_sent.append(1)\n",
    "    else:\n",
    "        neg_sent.append(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df['neg_sent'] = neg_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = df.tweet\n",
    "y = df.neg_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Features: ', 12525)\n",
      "('Accuracy: ', 0.79833333333333334)\n"
     ]
    }
   ],
   "source": [
    "vect = CountVectorizer(ngram_range=(1, 3), min_df=2)\n",
    "tokenize_test(vect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Features: ', 12457)\n",
      "('Accuracy: ', 0.81333333333333335)\n"
     ]
    }
   ],
   "source": [
    "vect = CountVectorizer()\n",
    "tokenize_test(vect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Features: ', 12205)\n",
      "('Accuracy: ', 0.80166666666666664)\n"
     ]
    }
   ],
   "source": [
    "vect = CountVectorizer(stop_words='english')\n",
    "tokenize_test(vect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Features: ', 55282)\n",
      "('Accuracy: ', 0.81833333333333336)\n"
     ]
    }
   ],
   "source": [
    "vect = CountVectorizer(ngram_range=(1, 2), max_features=100000)\n",
    "tokenize_test(vect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Features: ', 9766)\n",
      "('Accuracy: ', 0.80333333333333334)\n"
     ]
    }
   ],
   "source": [
    "vect = CountVectorizer(ngram_range=(1, 2), min_df=2)\n",
    "tokenize_test(vect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.791666666667\n"
     ]
    }
   ],
   "source": [
    "y_test_binary = np.where(y_test==3, 1, 0)\n",
    "print(y_test_binary.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "estimators = [('cv', CountVectorizer()), ('nb', MultinomialNB())]\n",
    "pip = Pipeline(estimators)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('cv', CountVectorizer(analyzer=u'word', binary=False, decode_error=u'strict',\n",
       "        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "        strip_accents=None, token_pattern=u'(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "        tokenizer=None, vocabulary=None)), ('nb', MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stop = ['english', None]\n",
    "accents = ['unicode', None]\n",
    "ngram = [(1,1),(1,2), (1,3), (2,3)]\n",
    "case = [True, False]\n",
    "mx = [0.4, 0.6, 0.8, 1]\n",
    "a = [0, 0.2, 0.4, 0.6, 0.8, 1]\n",
    "\n",
    "params = {'cv__stop_words': stop,\n",
    "         'cv__ngram_range': ngram,\n",
    "         'cv__strip_accents': accents,\n",
    "         'cv__lowercase': case,\n",
    "         'cv__max_df': mx,\n",
    "         'nb__alpha': a}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gs = GridSearchCV(estimator = pip, param_grid = params,\n",
    "                  n_jobs=-1, verbose = True, cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = df.tweet\n",
    "y = df.sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 768 candidates, totalling 3840 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ds/anaconda/lib/python2.7/site-packages/sklearn/naive_bayes.py:699: RuntimeWarning: divide by zero encountered in log\n",
      "  self.feature_log_prob_ = (np.log(smoothed_fc) -\n",
      "/Users/ds/anaconda/lib/python2.7/site-packages/sklearn/naive_bayes.py:699: RuntimeWarning: divide by zero encountered in log\n",
      "  self.feature_log_prob_ = (np.log(smoothed_fc) -\n",
      "/Users/ds/anaconda/lib/python2.7/site-packages/sklearn/naive_bayes.py:699: RuntimeWarning: divide by zero encountered in log\n",
      "  self.feature_log_prob_ = (np.log(smoothed_fc) -\n",
      "/Users/ds/anaconda/lib/python2.7/site-packages/sklearn/naive_bayes.py:699: RuntimeWarning: divide by zero encountered in log\n",
      "  self.feature_log_prob_ = (np.log(smoothed_fc) -\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:    5.5s\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:   33.9s\n",
      "[Parallel(n_jobs=-1)]: Done 442 tasks      | elapsed:  2.2min\n",
      "[Parallel(n_jobs=-1)]: Done 792 tasks      | elapsed:  3.7min\n",
      "[Parallel(n_jobs=-1)]: Done 1242 tasks      | elapsed:  5.9min\n",
      "[Parallel(n_jobs=-1)]: Done 1792 tasks      | elapsed:  8.9min\n",
      "[Parallel(n_jobs=-1)]: Done 2442 tasks      | elapsed: 12.2min\n",
      "[Parallel(n_jobs=-1)]: Done 3192 tasks      | elapsed: 15.9min\n",
      "[Parallel(n_jobs=-1)]: Done 3840 out of 3840 | elapsed: 19.5min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise',\n",
       "       estimator=Pipeline(steps=[('cv', CountVectorizer(analyzer=u'word', binary=False, decode_error=u'strict',\n",
       "        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "        strip_accents=None, token_pattern=u'(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "        tokenizer=None, vocabulary=None)), ('nb', MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))]),\n",
       "       fit_params={}, iid=True, n_jobs=-1,\n",
       "       param_grid={'cv__lowercase': [True, False], 'nb__alpha': [0, 0.2, 0.4, 0.6, 0.8, 1], 'cv__stop_words': ['english', None], 'cv__ngram_range': [(1, 1), (1, 2), (1, 3), (2, 3)], 'cv__strip_accents': ['unicode', None], 'cv__max_df': [0.4, 0.6, 0.8, 1]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
       "       scoring=None, verbose=True)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Params:\n",
      "{'cv__stop_words': None, 'cv__lowercase': True, 'nb__alpha': 0.8, 'cv__ngram_range': (1, 2), 'cv__strip_accents': 'unicode', 'cv__max_df': 0.4}\n"
     ]
    }
   ],
   "source": [
    "print \"Best Params:\"\n",
    "print gs.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "accents = ['unicode']\n",
    "ngram = [(1,2)]\n",
    "mx = [0.3, 0.4, 0.5]\n",
    "a = [0.7, 0.8, 0.9]\n",
    "\n",
    "params = {'cv__ngram_range': ngram,\n",
    "         'cv__strip_accents': accents,\n",
    "         'cv__max_df': mx,\n",
    "         'nb__alpha': a}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gs = GridSearchCV(estimator = pip, param_grid = params,\n",
    "                  n_jobs=-1, verbose = True, cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 9 candidates, totalling 45 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  45 out of  45 | elapsed:   14.6s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise',\n",
       "       estimator=Pipeline(steps=[('cv', CountVectorizer(analyzer=u'word', binary=False, decode_error=u'strict',\n",
       "        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "        strip_accents=None, token_pattern=u'(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "        tokenizer=None, vocabulary=None)), ('nb', MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))]),\n",
       "       fit_params={}, iid=True, n_jobs=-1,\n",
       "       param_grid={'cv__strip_accents': ['unicode'], 'cv__ngram_range': [(1, 2)], 'nb__alpha': [0.7, 0.8, 0.9], 'cv__max_df': [0.3, 0.4, 0.5]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
       "       scoring=None, verbose=True)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'cv__strip_accents': 'unicode', 'cv__ngram_range': (1, 2), 'nb__alpha': 0.8, 'cv__max_df': 0.3}\n"
     ]
    }
   ],
   "source": [
    "print gs.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "accents = ['unicode']\n",
    "ngram = [(1,2)]\n",
    "mx = [0.25,0.3, 0.35]\n",
    "a = [0.75, 0.8, 0.85]\n",
    "\n",
    "params = {'cv__ngram_range': ngram,\n",
    "         'cv__strip_accents': accents,\n",
    "         'cv__max_df': mx,\n",
    "         'nb__alpha': a}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gs = GridSearchCV(estimator = pip, param_grid = params,\n",
    "                  n_jobs=-1, verbose = True, cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 9 candidates, totalling 45 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  45 out of  45 | elapsed:   15.5s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise',\n",
       "       estimator=Pipeline(steps=[('cv', CountVectorizer(analyzer=u'word', binary=False, decode_error=u'strict',\n",
       "        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "        strip_accents=None, token_pattern=u'(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "        tokenizer=None, vocabulary=None)), ('nb', MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))]),\n",
       "       fit_params={}, iid=True, n_jobs=-1,\n",
       "       param_grid={'cv__strip_accents': ['unicode'], 'cv__ngram_range': [(1, 2)], 'nb__alpha': [0.75, 0.8, 0.85], 'cv__max_df': [0.25, 0.3, 0.35]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
       "       scoring=None, verbose=True)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'cv__strip_accents': 'unicode', 'cv__ngram_range': (1, 2), 'nb__alpha': 0.8, 'cv__max_df': 0.35}\n"
     ]
    }
   ],
   "source": [
    "print gs.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tokenize_test(vect):\n",
    "    X_train_dtm = vect.fit_transform(X_train)\n",
    "    print('Features: ', X_train_dtm.shape[1])\n",
    "    X_test_dtm = vect.transform(X_test)\n",
    "    nb = MultinomialNB(alpha=0.8)\n",
    "    nb.fit(X_train_dtm, y_train)\n",
    "    y_pred_class = nb.predict(X_test_dtm)\n",
    "    print('Accuracy: ', metrics.accuracy_score(y_test, y_pred_class))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vect = CountVectorizer(strip_accents='unicode', ngram_range=(1,2), max_df=0.35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = df.tweet\n",
    "y = df.sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Features: ', 55270)\n",
      "('Accuracy: ', 0.66333333333333333)\n"
     ]
    }
   ],
   "source": [
    "tokenize_test(vect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stop = ['english', None]\n",
    "accents = ['unicode', None]\n",
    "ngram = [(1,1),(1,2), (1,3), (2,3)]\n",
    "case = [True, False]\n",
    "mx = [0.4, 0.6, 0.8, 1]\n",
    "a = [0, 0.2, 0.4, 0.6, 0.8, 1]\n",
    "\n",
    "params = {'cv__stop_words': stop,\n",
    "         'cv__ngram_range': ngram,\n",
    "         'cv__strip_accents': accents,\n",
    "         'cv__lowercase': case,\n",
    "         'cv__max_df': mx,\n",
    "         'nb__alpha': a}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = pos_neg.tweet\n",
    "y = pos_neg.sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gs = GridSearchCV(estimator = pip, param_grid = params,\n",
    "                  n_jobs=-1, verbose = True, cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 768 candidates, totalling 3840 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ds/anaconda/lib/python2.7/site-packages/sklearn/naive_bayes.py:699: RuntimeWarning: divide by zero encountered in log\n",
      "  self.feature_log_prob_ = (np.log(smoothed_fc) -\n",
      "/Users/ds/anaconda/lib/python2.7/site-packages/sklearn/naive_bayes.py:699: RuntimeWarning: divide by zero encountered in log\n",
      "  self.feature_log_prob_ = (np.log(smoothed_fc) -\n",
      "/Users/ds/anaconda/lib/python2.7/site-packages/sklearn/naive_bayes.py:699: RuntimeWarning: divide by zero encountered in log\n",
      "  self.feature_log_prob_ = (np.log(smoothed_fc) -\n",
      "/Users/ds/anaconda/lib/python2.7/site-packages/sklearn/naive_bayes.py:699: RuntimeWarning: divide by zero encountered in log\n",
      "  self.feature_log_prob_ = (np.log(smoothed_fc) -\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:    3.6s\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:   18.8s\n",
      "[Parallel(n_jobs=-1)]: Done 442 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=-1)]: Done 792 tasks      | elapsed:  2.0min\n",
      "[Parallel(n_jobs=-1)]: Done 1242 tasks      | elapsed:  3.1min\n",
      "[Parallel(n_jobs=-1)]: Done 1792 tasks      | elapsed:  4.7min\n",
      "[Parallel(n_jobs=-1)]: Done 2442 tasks      | elapsed:  6.5min\n",
      "[Parallel(n_jobs=-1)]: Done 3192 tasks      | elapsed:  8.7min\n",
      "[Parallel(n_jobs=-1)]: Done 3840 out of 3840 | elapsed: 10.8min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise',\n",
       "       estimator=Pipeline(steps=[('cv', CountVectorizer(analyzer=u'word', binary=False, decode_error=u'strict',\n",
       "        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "        strip_accents=None, token_pattern=u'(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "        tokenizer=None, vocabulary=None)), ('nb', MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))]),\n",
       "       fit_params={}, iid=True, n_jobs=-1,\n",
       "       param_grid={'cv__lowercase': [True, False], 'nb__alpha': [0, 0.2, 0.4, 0.6, 0.8, 1], 'cv__stop_words': ['english', None], 'cv__ngram_range': [(1, 1), (1, 2), (1, 3), (2, 3)], 'cv__strip_accents': ['unicode', None], 'cv__max_df': [0.4, 0.6, 0.8, 1]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
       "       scoring=None, verbose=True)"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'cv__stop_words': None, 'cv__lowercase': True, 'nb__alpha': 0.4, 'cv__ngram_range': (1, 2), 'cv__strip_accents': 'unicode', 'cv__max_df': 0.4}\n"
     ]
    }
   ],
   "source": [
    "print gs.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
